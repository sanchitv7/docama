# ğŸ“„ Doc-AMA â€“ Ask Me Anything from a PDF

Doc-AMA is a lightweight app that lets you upload a PDF and ask natural language questions about its content.
It combines local PDF parsing, semantic chunking, vector similarity search, and LLM-powered answers using **open-access models** like Windsurf or OpenRouter.

---

## ğŸŒŸ Features

- ğŸ—‚ Upload any PDF file and extract clean text
- ğŸ” Chunk and embed the document using Sentence Transformers
- ğŸ¤– Ask questions about the PDF â€” get smart, context-aware answers
- ğŸ’¡ Uses retrieval-augmented generation (RAG) with FAISS or Chroma
- âš™ï¸ Switch between OpenRouter, Ollama, or other backends
- ğŸ§± Modular codebase (parser, embedder, retriever, LLM handler)

---

## ğŸ§  Why I Built This

I wanted to deeply understand how modern RAG (retrieval-augmented generation) works â€” how to go from unstructured documents to semantically meaningful answers powered by LLMs.
Doc-AMA is built to be clean, hackable, and open â€” and to help me grow beyond vibe coding into system-level thinking.

---

## ğŸ§° Tech Stack

| Layer | Tool |
|-------|------|
| ğŸ§¾ PDF Parsing | `PyMuPDF` (`fitz`) |
| ğŸ§  Embeddings | `sentence-transformers` (e.g., MiniLM) |
| ğŸ” Vector Store | `FAISS` or `Chroma` |
| ğŸ’¬ LLM Backend | `OpenRouter`, `Ollama`, or `Windsurf` |
| ğŸ§‘â€ğŸ’» UI | `Gradio` |
| ğŸ”„ Deployment | Hugging Face Spaces |

---

## ğŸ§ª Running Locally

### 1. Install dependencies (using [uv](https://github.com/astral-sh/uv))
```bash
uv venv
source .venv/bin/activate
uv pip install -r requirements.txt
```

### 2. Set required environment variables
Create a `.env` file in the project root (or export variables in your shell) with the following keys:

```bash
# OpenRouter (https://openrouter.ai)
OPENROUTER_API_KEY=sk-...

# Ollama (https://ollama.com) â€“ only needed if you choose the Ollama backend
# For local Ollama use the default below
OLLAMA_BASE_URL=http://localhost:11434

# Windsurf (https://github.com/pygmalion-ai/windsurf) â€“ optional
WINDSURF_BASE_URL=http://localhost:8000
```

You can mix-and-match providers at runtime via the **Settings** panel in the UI. If a backend is not configured, it will be hidden automatically.

### 3. Launch the app
```bash
python app.py
```
By default the Gradio interface will be available at <http://localhost:7860>.

---

## âš™ï¸ Configuration
Key settings live in `config.py` and can be tweaked without touching the core code. Notable options:

| Setting | Default | Description |
|---------|---------|-------------|
| `CHUNK_SIZE` | `1024` tokens | Maximum chunk size passed to the embedder |
| `VECTOR_DB` | `"faiss"` | Choose between `faiss` (in-memory) or `chroma` (persistent) |
| `EMBED_MODEL_NAME` | `"sentence-transformers/all-MiniLM-L6-v2"` | Hugging Face model used for embeddings |
| `MAX_CONTEXT_CHUNKS` | `5` | How many relevant chunks to stuff into the LLM prompt |

Changes are picked up on the next run.

---

## ğŸ“ Project Structure
```text
docama/
â”œâ”€â”€ app.py            # Gradio UI & orchestrator
â”œâ”€â”€ config.py         # Central configuration
â”œâ”€â”€ embedder.py       # Sentence-Transformers wrapper
â”œâ”€â”€ llm.py            # Thin client for LLM backends (OpenRouter, Ollama, Windsurf)
â”œâ”€â”€ parser.py         # PDF text extraction & chunking
â”œâ”€â”€ retriever.py      # FAISS/Chroma similarity search
â”œâ”€â”€ utils.py          # Helper functions
â””â”€â”€ README.md         # You are here ğŸ‰
```
The `src/docama` directory is reserved for packaging if you wish to `pip install` the project in the future.

---

## ğŸš€ Deployment
The easiest way to share Doc-AMA is via **Hugging Face Spaces**:
1. Push this repository to your HF account.
2. In the Space settings, choose the **Gradio** runtime.
3. Set the same environment variables as in the `.env` file (under *Variables & secrets*).
4. Click **Restart Space** â€“ done!

For containerised deployments you can adapt the provided `Dockerfile` example (coming soon) or rely on services like Fly.io, Render.com, or AWS App Runner.

---

> *Built with <3 by [YOUR NAME] â€” happy hacking!*